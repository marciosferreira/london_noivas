#!/usr/bin/env python3
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘       ğŸ¤– AGENTE ReAct MODERNO COM MCP TOOLS (DIDÃTICO - 2025)                  â•‘
â•‘                      VERSÃƒO OPENAI (function calling nativo)                   â•‘
â•‘                                                                                â•‘
â•‘  PadrÃµes atuais da indÃºstria:                                                  â•‘
â•‘                                                                                â•‘
â•‘  âœ… FUNCTION CALLING NATIVO da API OpenAI                                      â•‘
â•‘     A LLM retorna tool_calls[] estruturados â€” sem text parsing.                â•‘
â•‘     Equivalente ao tool_use da Anthropic.                                      â•‘
â•‘                                                                                â•‘
â•‘  âœ… MULTI-TURN NATURAL = SCRATCHPAD                                            â•‘
â•‘     O histÃ³rico de mensagens Ã‰ a memÃ³ria de trabalho:                          â•‘
â•‘     user â†’ assistant(tool_calls) â†’ tool(result) â†’ assistant(tool_calls) â†’ ...  â•‘
â•‘                                                                                â•‘
â•‘  âœ… COMPRESSÃƒO DE CONTEXTO COM LLM                                             â•‘
â•‘     Quando o histÃ³rico cresce demais, a LLM gera um resumo.                   â•‘
â•‘                                                                                â•‘
â•‘  âœ… MCP VIA STDIO (JSON-RPC 2.0)                                               â•‘
â•‘     Protocolo padrÃ£o para ferramentas de LLM.                                 â•‘
â•‘                                                                                â•‘
â•‘  COMPARAÃ‡ÃƒO: OPENAI vs ANTHROPIC (function calling nativo):                    â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â•‘
â•‘  â”‚ Aspecto          â”‚ OpenAI                  â”‚ Anthropic                â”‚      â•‘
â•‘  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â•‘
â•‘  â”‚ Tool schema      â”‚ type:"function",        â”‚ name, input_schema       â”‚      â•‘
â•‘  â”‚                  â”‚ function:{parameters}   â”‚                          â”‚      â•‘
â•‘  â”‚ LLM pede tool    â”‚ finish_reason=          â”‚ stop_reason=             â”‚      â•‘
â•‘  â”‚                  â”‚   "tool_calls"          â”‚   "tool_use"             â”‚      â•‘
â•‘  â”‚ Dados da tool    â”‚ tool_calls[].function   â”‚ ToolUseBlock             â”‚      â•‘
â•‘  â”‚                  â”‚   .name, .arguments     â”‚   .name, .input          â”‚      â•‘
â•‘  â”‚ Resultado        â”‚ role="tool",            â”‚ role="user",             â”‚      â•‘
â•‘  â”‚                  â”‚ tool_call_id=...        â”‚ type="tool_result"       â”‚      â•‘
â•‘  â”‚ LLM terminou     â”‚ finish_reason="stop"    â”‚ stop_reason="end_turn"   â”‚      â•‘
â•‘  â”‚ Args format      â”‚ JSON STRING (precisa    â”‚ DICT nativo              â”‚      â•‘
â•‘  â”‚                  â”‚   json.loads)           â”‚                          â”‚      â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â•‘
â•‘                                                                                â•‘
â•‘  Requisitos: pip install langfuse                                              â•‘
â•‘  Uso: OPENAI_API_KEY=sk-... python react_agent_modern.py                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CONCEITOS-CHAVE:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conceito             â”‚ Onde no cÃ³digo      â”‚ Por que importa                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Function Calling     â”‚ process()          â”‚ API retorna JSON, nÃ£o texto    â”‚
â”‚ finish_reason        â”‚ process()          â”‚ API diz quando parar           â”‚
â”‚ Multi-turn           â”‚ messages array     â”‚ HistÃ³rico = scratchpad         â”‚
â”‚ role="tool"          â”‚ process()          â”‚ Resultados das tools           â”‚
â”‚ tool_call_id         â”‚ process()          â”‚ Vincula resultado â†’ chamada    â”‚
â”‚ MCP Protocol         â”‚ MCPClient          â”‚ PadrÃ£o de ferramentas          â”‚
â”‚ LLM Compression      â”‚ _compress()        â”‚ Contexto infinito viÃ¡vel       â”‚
â”‚ Schema conversion    â”‚ mcp_to_openai()    â”‚ Ponte MCP â†” OpenAI             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FLUXO DO FUNCTION CALLING NATIVO (OpenAI):

  ANTES (text parsing ReAct):          AGORA (function calling nativo):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ System prompt: 800 tok  â”‚           â”‚ System prompt: 150 tok         â”‚
  â”‚ "Responda no formato:   â”‚           â”‚ "Resolva passo a passo"        â”‚
  â”‚  Thought: ...            â”‚           â”‚ (formato Ã© da API, nÃ£o nosso!) â”‚
  â”‚  Action: ...             â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚  Action Input: ..."      â”‚                          â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
             â”‚                                          â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ LLM retorna TEXTO       â”‚           â”‚ LLM retorna OBJETOS            â”‚
  â”‚ "Thought: preciso somar â”‚           â”‚ message.tool_calls = [          â”‚
  â”‚  Action: somar           â”‚           â”‚   {id: "call_abc",             â”‚
  â”‚  Action Input: {a:1,b:2}"â”‚           â”‚    function: {                  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚      name: "somar",            â”‚
             â”‚                           â”‚      arguments: '{"a":1,"b":2}'â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚    }}]                          â”‚
  â”‚ PARSING COM STRING      â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚ if "Action:" in text... â”‚                          â”‚
  â”‚ json.loads(...)  â† FRÃGILâ”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚ json.loads(arguments) â† ÃšNICO   â”‚
                                         â”‚ parse necessÃ¡rio, e Ã© confiÃ¡velâ”‚
                                         â”‚ porque a API garante formato   â”‚
                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

import json
import subprocess
import sys
import os
from typing import Optional


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  PARTE 1: SERVIDOR MCP (Model Context Protocol) via STDIO
#
#  Protocolo padrÃ£o para ferramentas de LLM.
#  JSON-RPC 2.0 sobre stdin/stdout do subprocesso.
#  MÃ©todos: initialize â†’ tools/list â†’ tools/call
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MCP_SERVER_CODE = r'''
#!/usr/bin/env python3
"""Servidor MCP (stdio) â€” ferramentas de soma e subtraÃ§Ã£o."""
import sys, json

def handle(req):
    method, rid, params = req.get("method",""), req.get("id"), req.get("params",{})

    if method == "initialize":
        return {"jsonrpc":"2.0","id":rid,"result":{
            "protocolVersion":"2024-11-05",
            "capabilities":{"tools":{}},
            "serverInfo":{"name":"calculadora-mcp","version":"1.0.0"}
        }}
    if method == "notifications/initialized":
        return None
    if method == "tools/list":
        return {"jsonrpc":"2.0","id":rid,"result":{"tools":[
            {"name":"somar",
             "description":"Soma dois nÃºmeros (a + b).",
             "inputSchema":{"type":"object","properties":{
                 "a":{"type":"number","description":"Primeiro nÃºmero"},
                 "b":{"type":"number","description":"Segundo nÃºmero"}
             },"required":["a","b"]}},
            {"name":"subtrair",
             "description":"Subtrai b de a (a - b).",
             "inputSchema":{"type":"object","properties":{
                 "a":{"type":"number","description":"Minuendo"},
                 "b":{"type":"number","description":"Subtraendo"}
             },"required":["a","b"]}}
        ]}}
    if method == "tools/call":
        name = params.get("name","")
        args = params.get("arguments",{})
        a, b = args.get("a",0), args.get("b",0)
        if name == "somar":
            return {"jsonrpc":"2.0","id":rid,"result":{"content":[
                {"type":"text","text":f"{a} + {b} = {a+b}"}],"isError":False}}
        if name == "subtrair":
            return {"jsonrpc":"2.0","id":rid,"result":{"content":[
                {"type":"text","text":f"{a} - {b} = {a-b}"}],"isError":False}}
        return {"jsonrpc":"2.0","id":rid,"result":{"content":[
            {"type":"text","text":f"Tool '{name}' nÃ£o encontrada"}],"isError":True}}
    return {"jsonrpc":"2.0","id":rid,"error":{"code":-32601,"message":f"MÃ©todo: {method}"}}

for line in sys.stdin:
    line = line.strip()
    if not line: continue
    try:
        resp = handle(json.loads(line))
        if resp: print(json.dumps(resp), flush=True)
    except Exception as e:
        print(f"[MCP ERROR] {e}", file=sys.stderr)
'''


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  PARTE 2: CLIENTE MCP
#
#  Gerencia subprocesso do servidor e comunicaÃ§Ã£o JSON-RPC.
#  Ciclo: start() â†’ initialize() â†’ list_tools() â†’ call_tool() Ã— N â†’ stop()
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class MCPClient:
    """Cliente MCP via STDIO (subprocess)."""

    def __init__(self, server_path: str):
        self.server_path = server_path
        self.process: Optional[subprocess.Popen] = None
        self._req_id = 0

    def start(self):
        print("  ğŸ”Œ Iniciando servidor MCP...")
        self.process = subprocess.Popen(
            [sys.executable, self.server_path],
            stdin=subprocess.PIPE, stdout=subprocess.PIPE,
            stderr=subprocess.PIPE, text=True, bufsize=1,
        )
        print(f"  âœ… MCP rodando (PID: {self.process.pid})")

    def _send(self, method: str, params: dict = None) -> Optional[dict]:
        self._req_id += 1
        req = {"jsonrpc": "2.0", "id": self._req_id, "method": method}
        if params:
            req["params"] = params
        self.process.stdin.write(json.dumps(req) + "\n")
        self.process.stdin.flush()
        if method.startswith("notifications/"):
            return None
        line = self.process.stdout.readline()
        if not line:
            raise RuntimeError("MCP nÃ£o respondeu")
        return json.loads(line.strip())

    def initialize(self):
        res = self._send("initialize", {
            "protocolVersion": "2024-11-05",
            "capabilities": {},
            "clientInfo": {"name": "react-agent", "version": "2.0.0"},
        })
        self._send("notifications/initialized")
        name = res["result"]["serverInfo"]["name"]
        print(f"  ğŸ¤ Handshake OK â€” servidor: {name}")
        return res

    def list_tools(self) -> list:
        res = self._send("tools/list")
        tools = res["result"]["tools"]
        print(f"  ğŸ”§ {len(tools)} tools:")
        for t in tools:
            print(f"     â€¢ {t['name']}: {t['description']}")
        return tools

    def call_tool(self, name: str, arguments: dict) -> str:
        res = self._send("tools/call", {"name": name, "arguments": arguments})
        return " ".join(
            c["text"] for c in res["result"]["content"] if c["type"] == "text"
        )

    def stop(self):
        if self.process:
            self.process.terminate()
            self.process.wait(timeout=5)
            print("  ğŸ›‘ MCP encerrado")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  PARTE 3: CONVERSÃƒO MCP â†’ OPENAI TOOL SCHEMA
#
#  O MCP retorna tools no formato MCP:
#    {"name": "somar", "description": "...", "inputSchema": {...}}
#
#  A API OpenAI espera tools no formato OpenAI:
#    {"type": "function", "function": {"name": "somar", "description": "...",
#     "parameters": {...}}}
#
#  A diferenÃ§a principal:
#    MCP: inputSchema (camelCase, no nÃ­vel raiz)
#    OpenAI: parameters (dentro de "function", envelopado em "type":"function")
#
#  Na Anthropic seria: input_schema (snake_case, no nÃ­vel raiz)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def mcp_to_openai_tools(mcp_tools: list[dict]) -> list[dict]:
    """
    Converte tools do formato MCP para o formato da API OpenAI.

    MCP:    {"name": "somar", "description": "...", "inputSchema": {...}}
    OpenAI: {"type": "function", "function": {"name": "somar",
             "description": "...", "parameters": {...}}}
    """
    openai_tools = []
    for t in mcp_tools:
        openai_tools.append({
            "type": "function",
            "function": {
                "name": t["name"],
                "description": t["description"],
                "parameters": t["inputSchema"],  # inputSchema â†’ parameters
            }
        })
    return openai_tools


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  PARTE 4: FUNÃ‡Ã•ES DE DEBUG / PRINTS DIDÃTICOS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def print_header(title: str, char: str = "â”€", width: int = 70):
    print(f"\n  {char*3} {title} {char * max(1, width - len(title) - 5)}")


def print_messages(messages: list[dict], label: str = "MESSAGES"):
    """
    Mostra o array de mensagens de forma legÃ­vel.

    No formato OpenAI, as mensagens podem ter:
    - role="system"    â†’ system prompt
    - role="user"      â†’ mensagem do usuÃ¡rio
    - role="assistant" â†’ resposta da LLM (pode ter tool_calls)
    - role="tool"      â†’ resultado de uma ferramenta
    """
    print(f"\n  â”Œ{'â”€'*68}â”")
    print(f"  â”‚ ğŸ“¤ {label[:64]:64s}â”‚")
    print(f"  â”œ{'â”€'*68}â”¤")

    for i, msg in enumerate(messages):
        role = msg["role"].upper()
        content = msg.get("content") or ""

        # Mensagem com tool_calls (assistant pedindo tools)
        tool_calls = msg.get("tool_calls")
        if tool_calls:
            for tc in tool_calls:
                fn = tc["function"]
                args_preview = fn["arguments"][:35]
                print(f"  â”‚ [{i:2d}] {role:10s} ğŸ”§ {fn['name']}({args_preview}){' '*15}â”‚")
            # TambÃ©m pode ter texto
            if content:
                preview = str(content)[:45].replace("\n", " ")
                print(f"  â”‚      {'':10s} ğŸ’¬ {preview:50s}â”‚")

        # Mensagem role="tool" (resultado)
        elif role == "TOOL":
            tid = msg.get("tool_call_id", "?")[:10]
            preview = str(content)[:40]
            print(f"  â”‚ [{i:2d}] {role:10s} ğŸ“¥ [{tid}]: {preview:33s}â”‚")

        # Mensagem simples de texto (user, system, assistant sem tools)
        else:
            preview = str(content)[:55].replace("\n", " â†µ ")
            print(f"  â”‚ [{i:2d}] {role:10s} ğŸ’¬ {preview:50s}â”‚")

    print(f"  â””{'â”€'*68}â”˜")


def print_api_response(response_data: dict):
    """
    Mostra a resposta da API OpenAI de forma didÃ¡tica.

    Estrutura da resposta OpenAI:
    {
      "choices": [{
        "message": {
          "role": "assistant",
          "content": "texto" | null,
          "tool_calls": [{              â† SÃ“ EXISTE SE LLM QUER USAR TOOL
            "id": "call_abc123",
            "type": "function",
            "function": {
              "name": "somar",
              "arguments": '{"a":1,"b":2}'   â† JSON STRING, nÃ£o dict!
            }
          }]
        },
        "finish_reason": "stop" | "tool_calls"
      }],
      "usage": {"prompt_tokens": N, "completion_tokens": N, "total_tokens": N}
    }
    """
    choice = response_data["choices"][0]
    message = choice["message"]
    finish = choice["finish_reason"]
    usage = response_data.get("usage", {})

    print(f"\n  â”Œ{'â”€'*68}â”")
    print(f"  â”‚ ğŸ“¥ RESPOSTA DA API OPENAI                                          â”‚")
    print(f"  â”œ{'â”€'*68}â”¤")
    print(f"  â”‚ finish_reason: {finish:51s} â”‚")
    usage_str = f"prompt={usage.get('prompt_tokens',0)} completion={usage.get('completion_tokens',0)} total={usage.get('total_tokens',0)}"
    print(f"  â”‚ usage: {usage_str:60s} â”‚")
    print(f"  â”œ{'â”€'*68}â”¤")

    # Texto da resposta
    content = message.get("content")
    if content:
        for line in content.split("\n")[:5]:
            preview = line[:62]
            print(f"  â”‚ ğŸ’¬ {preview:64s}â”‚")

    # Tool calls
    tool_calls = message.get("tool_calls")
    if tool_calls:
        for tc in tool_calls:
            fn = tc["function"]
            print(f"  â”‚ ğŸ”§ TOOL_CALL                                                     â”‚")
            print(f"  â”‚    id:        {tc['id']:53s}â”‚")
            print(f"  â”‚    name:      {fn['name']:53s}â”‚")
            print(f"  â”‚    arguments: {fn['arguments'][:53]:53s}â”‚")

    print(f"  â””{'â”€'*68}â”˜")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  PARTE 5: O AGENTE
#
#  ARQUITETURA (OpenAI function calling nativo):
#
#   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#   â”‚ UsuÃ¡rio â”‚â”€â”€â”€â”€â–¶â”‚  process()     â”‚â”€â”€â”€â”€â–¶â”‚ OpenAI API   â”‚
#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚                â”‚     â”‚ tools=[...]  â”‚
#                   â”‚  messages = [  â”‚     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
#                   â”‚    system,     â”‚            â”‚
#                   â”‚    user,       â”‚     finish_reason
#                   â”‚    assistant,  â”‚       â”‚
#                   â”‚    tool,       â”‚       â”œâ”€ "tool_calls" â†’ executa MCP
#                   â”‚    assistant,  â”‚       â”‚                  appenda msgs
#                   â”‚    tool,       â”‚       â”‚                  volta ao loop
#                   â”‚    ...         â”‚       â”‚
#                   â”‚  ]             â”‚       â””â”€ "stop" â†’ extrai texto
#                   â”‚                â”‚                   retorna
#                   â”‚  O ARRAY       â”‚
#                   â”‚  MESSAGES      â”‚
#                   â”‚  Ã‰ O           â”‚
#                   â”‚  SCRATCHPAD    â”‚
#                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
#  DIFERENÃ‡A OPENAI vs ANTHROPIC no role de tool results:
#
#  OpenAI:    role="tool", tool_call_id="call_abc"
#  Anthropic: role="user", content=[{type:"tool_result", tool_use_id:"toolu_abc"}]
#
#  Na OpenAI, cada resultado Ã© uma MENSAGEM SEPARADA com role="tool".
#  Na Anthropic, todos os resultados vÃ£o em UMA mensagem role="user".
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class Agent:
    """Agente ReAct com function calling nativo da API OpenAI."""

    MAX_ITERATIONS = 15

    # CompressÃ£o do turn: muitas tool calls em UMA pergunta
    MAX_TURN_MESSAGES = 24

    # CompressÃ£o do histÃ³rico: conversa longa entre perguntas
    MAX_HISTORY_MESSAGES = 6

    def __init__(self, api_key: str):
        if sys.version_info >= (3, 14):
            print("âš ï¸ Langfuse usa Pydantic v1 e nÃ£o suporta Python 3.14.")
            print("   Use Python 3.13/3.12 para habilitar tracing.")
        try:
            from langfuse.openai import openai as langfuse_openai
            langfuse_openai.api_key = api_key
            self.client = langfuse_openai
        except Exception as exc:
            import openai as openai_module
            print("âš ï¸ Langfuse OpenAI falhou, usando OpenAI direto.")
            print(f"   Motivo: {exc}")
            openai_module.api_key = api_key
            self.client = openai_module
        self.model = "gpt-4o-mini"

        self.mcp: Optional[MCPClient] = None
        self.tools: list[dict] = []  # Formato OpenAI

        # HistÃ³rico persistente entre perguntas
        self.history: list[dict] = []

    # â”€â”€ Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def setup(self):
        """Inicia MCP e converte tools para formato OpenAI."""
        print("\nğŸ”§ Setup...")
        path = "/tmp/mcp_calc_server.py"
        with open(path, "w", encoding="utf-8") as f:
            f.write(MCP_SERVER_CODE)

        self.mcp = MCPClient(path)
        self.mcp.start()
        self.mcp.initialize()

        mcp_tools = self.mcp.list_tools()

        # â”€â”€ CONVERSÃƒO MCP â†’ OPENAI â”€â”€
        self.tools = mcp_to_openai_tools(mcp_tools)

        print("\n  ğŸ“‹ Tools no formato OpenAI:")
        for t in self.tools:
            print(f"     {json.dumps(t, ensure_ascii=False, indent=6)}")
        print()

    # â”€â”€ System Prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #
    # NOTA: Mesmo com OpenAI, o system prompt Ã© simples.
    # As tools sÃ£o declaradas via parÃ¢metro `tools=` na API,
    # nÃ£o precisamos descrevÃª-las no prompt.

    SYSTEM_PROMPT = (
        "VocÃª Ã© um assistente que resolve problemas matemÃ¡ticos passo a passo.\n\n"
        "Regras:\n"
        "- Use as ferramentas para TODOS os cÃ¡lculos â€” nunca calcule mentalmente\n"
        "- FaÃ§a UMA operaÃ§Ã£o por vez, usando o resultado da anterior\n"
        "- Explique brevemente seu raciocÃ­nio antes de cada operaÃ§Ã£o\n"
        "- Converse em portuguÃªs\n"
        "- Ao terminar, dÃª a resposta final de forma clara"
    )

    # â”€â”€ CompressÃ£o com LLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _compress_with_llm(self, messages: list[dict]) -> str:
        """
        Usa a prÃ³pria LLM para comprimir histÃ³rico de mensagens.

        QUANDO:
        1. Turn compression: loop de tools de UMA pergunta > MAX_TURN_MESSAGES
        2. History compression: conversa total > MAX_HISTORY_MESSAGES

        POR QUE LLM E NÃƒO REGRAS:
        - Preserva nuances que regras perdem
        - Sabe quais nÃºmeros/resultados importam
        - Gera resumos que a LLM entende bem depois
        """
        parts = []
        for msg in messages:
            role = msg["role"]
            content = msg.get("content") or ""

            # Mensagem com tool_calls (assistant)
            tool_calls = msg.get("tool_calls")
            if tool_calls:
                for tc in tool_calls:
                    fn = tc["function"]
                    parts.append(f"[assistant â†’ tool]: {fn['name']}({fn['arguments']})")
                if content:
                    parts.append(f"[assistant]: {str(content)[:300]}")

            # Resultado de tool
            elif role == "tool":
                parts.append(f"[tool resultado]: {str(content)[:200]}")

            # Mensagem normal
            elif content:
                parts.append(f"[{role}]: {str(content)[:500]}")

        text = "\n".join(parts)
        print(f"\n  ğŸ“¦ COMPRESSÃƒO COM LLM:")
        print(f"     Input: {len(text)} chars, {len(messages)} msgs")

        try:
            compress_messages = [
                {"role": "system", "content": (
                    "Resuma o histÃ³rico abaixo em 2-3 frases objetivas. "
                    "PRESERVE todos os nÃºmeros e resultados. "
                    "NÃ£o invente informaÃ§Ãµes."
                )},
                {"role": "user", "content": text},
            ]
            compress_payload = {
                "model": self.model,
                "max_tokens": 300,
                "temperature": 0,
                "messages": compress_messages,
            }
            print("\n===== RAW REQUEST (COMPRESSÃƒO) BEGIN =====")
            print(json.dumps(compress_payload, ensure_ascii=False))
            print("===== RAW REQUEST (COMPRESSÃƒO) END =====\n")
            resp = self.client.chat.completions.create(
                model=self.model,
                max_tokens=300,
                temperature=0,
                messages=compress_messages,
            )
            resp_dict = resp.model_dump()
            print("\n===== RAW RESPONSE (COMPRESSÃƒO) BEGIN =====")
            print(json.dumps(resp_dict, ensure_ascii=False))
            print("===== RAW RESPONSE (COMPRESSÃƒO) END =====\n")
            summary = resp.choices[0].message.content
            print(f"     Output: {len(summary)} chars")
            print(f"     Resumo: {summary[:150]}...")
            return summary
        except Exception as e:
            print(f"     âŒ Erro: {e}")
            return "HistÃ³rico anterior indisponÃ­vel."

    def _maybe_compress_history(self):
        """Comprime histÃ³rico de conversa se ultrapassou limite."""
        if len(self.history) <= self.MAX_HISTORY_MESSAGES:
            return

        print_header("COMPRESSÃƒO DE HISTÃ“RICO", "âš¡")
        print(f"     Antes: {len(self.history)} msgs")

        keep = 6
        old = self.history[:-keep]
        recent = self.history[-keep:]

        summary = self._compress_with_llm(old)

        self.history = [
            {"role": "user", "content": f"Nossa conversa anterior:\n{summary}"},
            {"role": "assistant", "content": "Compreendo nossa conversa anterior. Como posso continuar ajudando?"},
        ] + recent

        print(f"     Depois: {len(self.history)} msgs")
        print_messages(self.history, "HISTÃ“RICO APÃ“S COMPRESSÃƒO")

    # â”€â”€ Loop Principal (function calling nativo) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def process(self, user_input: str) -> str:
        """
        Processa uma mensagem usando function calling NATIVO da OpenAI.

        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘  FLUXO DO FUNCTION CALLING (OpenAI)                           â•‘
        â•‘                                                                â•‘
        â•‘  1. messages = [system] + history + [user: pergunta]          â•‘
        â•‘  2. response = API(messages, tools)                           â•‘
        â•‘  3. finish_reason == "stop"?                                  â•‘
        â•‘       â†’ extrair content, retornar. FIM.                      â•‘
        â•‘  4. finish_reason == "tool_calls"?                            â•‘
        â•‘       â†’ para cada tool_call:                                  â•‘
        â•‘           â†’ json.loads(arguments)  â† Ãºnico parse necessÃ¡rio  â•‘
        â•‘           â†’ executar via MCP                                  â•‘
        â•‘           â†’ messages.append(role="assistant", tool_calls=...) â•‘
        â•‘           â†’ messages.append(role="tool", resultado)           â•‘
        â•‘       â†’ volta ao passo 2                                     â•‘
        â•‘                                                                â•‘
        â•‘  O array `messages` cresce â€” Ã‰ o scratchpad.                 â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """
        print(f"\n{'='*70}")
        print(f"  ğŸ“ Pergunta: \"{user_input}\"")
        print(f"{'='*70}")

        # Monta mensagens: system + histÃ³rico + nova pergunta
        messages = [{"role": "system", "content": self.SYSTEM_PROMPT}]
        messages.extend(self.history)
        messages.append({"role": "user", "content": user_input})

        # Guardar posiÃ§Ã£o inicial para contar msgs do turn
        turn_start = len(messages)

        print_header("MENSAGENS INICIAIS")
        print_messages(messages, "Estado inicial â†’ API")

        final_text = None
        iteration = 0

        for iteration in range(1, self.MAX_ITERATIONS + 1):
            print_header(f"ITERAÃ‡ÃƒO {iteration}/{self.MAX_ITERATIONS}")

            # â”€â”€ CompressÃ£o do turn se necessÃ¡rio â”€â”€
            turn_count = len(messages) - turn_start
            print(f"  ğŸ“Š Mensagens neste turn: {turn_count}")
            print(f"  ğŸ“Š Mensagens totais: {len(messages)}")

            if turn_count > self.MAX_TURN_MESSAGES:
                print(f"\n  âš ï¸  Turn com {turn_count} msgs > limite {self.MAX_TURN_MESSAGES}")

                # Manter: system + history + user original + Ãºltimas 4 msgs
                # Comprimir: tudo no meio
                keep_tail = 4
                fixed = messages[:turn_start + 1]  # AtÃ© e incluindo user original
                middle = messages[turn_start + 1 : -keep_tail]
                tail = messages[-keep_tail:]

                if middle:
                    summary = self._compress_with_llm(middle)
                    messages = (
                        fixed
                        + [{"role": "assistant", "content": f"[Resumo dos passos anteriores: {summary}]"}]
                        + [{"role": "user", "content": "Continue resolvendo a partir do resumo acima."}]
                        + tail
                    )
                    turn_start = len(fixed) - 1  # Ajusta referÃªncia
                    print(f"     ApÃ³s compressÃ£o: {len(messages)} msgs")

            # â”€â”€ Chama a API OpenAI â”€â”€
            print(f"\n  ğŸ“¤ Chamando API OpenAI...")
            print(f"     model: {self.model}")
            print(f"     tools: {[t['function']['name'] for t in self.tools]}")
            print(f"     messages: {len(messages)}")

            payload = {
                "model": self.model,
                "messages": messages,
                "tools": self.tools,
                "temperature": 0.2,
                "max_tokens": 1024,
            }
            print("\n===== RAW REQUEST (OpenAI) BEGIN =====")
            print(json.dumps(payload, ensure_ascii=False))
            print("===== RAW REQUEST (OpenAI) END =====\n")

            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                tools=self.tools,        # â† Tools declaradas nativamente!
                temperature=0.2,
                max_tokens=1024,
            )

            # Converte para dict para debug (o SDK retorna objetos Pydantic)
            response_dict = response.model_dump()
            print("\n===== RAW RESPONSE (OpenAI) BEGIN =====")
            print(json.dumps(response_dict, ensure_ascii=False))
            print("===== RAW RESPONSE (OpenAI) END =====\n")
            print_api_response(response_dict)

            choice = response.choices[0]
            assistant_msg = choice.message
            finish_reason = choice.finish_reason

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            #  DECISÃƒO: finish_reason
            #
            #  "stop"       â†’ LLM terminou. content = resposta final.
            #  "tool_calls" â†’ LLM quer usar tools. Executar e continuar.
            #
            #  Isso substitui COMPLETAMENTE o text parsing antigo.
            #  NÃ£o existe mais "procurar Final Answer: no texto".
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

            if finish_reason == "stop":
                # â”€â”€ RESPOSTA FINAL â”€â”€
                print(f"\n  âœ… finish_reason='stop' â†’ Resposta pronta!")
                final_text = assistant_msg.content or ""
                break

            elif finish_reason == "tool_calls":
                # â”€â”€ EXECUTAR TOOLS â”€â”€
                print(f"\n  ğŸ”„ finish_reason='tool_calls' â†’ Executando ferramenta(s)...")

                # 1) Append a mensagem do assistant (com tool_calls)
                #
                # NOTA: Na OpenAI, a mensagem do assistant que pede tools
                # deve ser preservada inteira no messages â€” incluindo os
                # tool_calls â€” senÃ£o a API retorna erro.
                assistant_dict = {
                    "role": "assistant",
                    "content": assistant_msg.content,  # Pode ser None ou texto
                    "tool_calls": [
                        {
                            "id": tc.id,
                            "type": "function",
                            "function": {
                                "name": tc.function.name,
                                "arguments": tc.function.arguments,
                            }
                        }
                        for tc in assistant_msg.tool_calls
                    ]
                }
                messages.append(assistant_dict)

                if assistant_msg.content:
                    print(f"     ğŸ’¬ LLM disse: \"{assistant_msg.content[:80]}\"")

                # 2) Executar cada tool via MCP
                for tc in assistant_msg.tool_calls:
                    fn_name = tc.function.name
                    fn_args_str = tc.function.arguments
                    call_id = tc.id

                    # NOTA: Na OpenAI, arguments Ã© uma JSON STRING, nÃ£o dict.
                    # Precisamos de json.loads(). Mas isso Ã© seguro porque
                    # a API garante JSON vÃ¡lido (diferente de text parsing).
                    fn_args = json.loads(fn_args_str)

                    print(f"\n     âš¡ MCP call: {fn_name}({json.dumps(fn_args)})")
                    print(f"        call_id: {call_id}")

                    observation = self.mcp.call_tool(fn_name, fn_args)
                    print(f"        ğŸ‘ï¸ Resultado: {observation}")

                    # 3) Append resultado como role="tool"
                    #
                    # DIFERENÃ‡A vs Anthropic:
                    #   OpenAI:    role="tool", tool_call_id="call_abc"
                    #   Anthropic: role="user", content=[{type:"tool_result"}]
                    #
                    # Na OpenAI, CADA resultado Ã© uma mensagem SEPARADA.
                    # Na Anthropic, todos vÃ£o em UMA mensagem.
                    messages.append({
                        "role": "tool",
                        "tool_call_id": call_id,
                        "content": observation,
                    })

                # Debug: estado das mensagens
                print_messages(messages, f"Mensagens apÃ³s iteraÃ§Ã£o {iteration}")

                continue  # Volta ao loop

            else:
                print(f"  âš ï¸ finish_reason inesperado: {finish_reason}")
                final_text = "Erro inesperado."
                break

        if final_text is None:
            final_text = f"NÃ£o resolvi em {self.MAX_ITERATIONS} iteraÃ§Ãµes."

        # â”€â”€ Salvar no histÃ³rico (sÃ³ pergunta + resposta, sem loop) â”€â”€
        self.history.append({"role": "user", "content": user_input})
        self.history.append({"role": "assistant", "content": final_text})

        # â”€â”€ Comprimir histÃ³rico se necessÃ¡rio â”€â”€
        self._maybe_compress_history()

        # â”€â”€ Stats â”€â”€
        print_header("STATS DO TURN")
        print(f"     IteraÃ§Ãµes: {iteration}")
        print(f"     Mensagens no turn: {len(messages) - turn_start}")
        print(f"     HistÃ³rico persistente: {len(self.history)} msgs")

        return final_text

    # â”€â”€ Loop interativo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def run(self):
        """Loop de conversa contÃ­nua."""
        print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     ğŸ¤– AGENTE ReAct MODERNO â€” OpenAI Function Calling (2025)   â•‘
â•‘                                                                  â•‘
â•‘  Exemplos:                                                       â•‘
â•‘  â€¢ "Pegue 100, some 50, tire 30, some 15"                      â•‘
â•‘  â€¢ "Quanto Ã© 42 + 18 - 7 + 100?"                               â•‘
â•‘  â€¢ "Comece com 1000, subtraia 10, some 10"                     â•‘
â•‘                                                                  â•‘
â•‘  Comandos: sair | stats | historico | limpar                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
        while True:
            try:
                inp = input("\nğŸ§‘ VocÃª: ").strip()
                if not inp:
                    continue
                if inp.lower() == "sair":
                    print("\nğŸ‘‹ AtÃ© logo!")
                    break
                if inp.lower() == "stats":
                    n = len(self.history)
                    chars = sum(len(str(m.get("content", ""))) for m in self.history)
                    print(f"\nğŸ“Š HistÃ³rico: {n} msgs, {chars} chars")
                    continue
                if inp.lower() == "historico":
                    print_messages(self.history, "HISTÃ“RICO PERSISTENTE")
                    continue
                if inp.lower() == "limpar":
                    self.history.clear()
                    print("\nğŸ§¹ Limpo!")
                    continue

                answer = self.process(inp)
                print(f"\n{'â”€'*70}")
                print(f"  ğŸ¤– Agente: {answer}")
                print(f"{'â”€'*70}")

            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ AtÃ© logo!")
                break
            except Exception as e:
                print(f"\nâŒ Erro: {e}")
                import traceback
                traceback.print_exc()

    def cleanup(self):
        if self.mcp:
            self.mcp.stop()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  PARTE 6: MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _load_dotenv():
    """Carrega variÃ¡veis de .env se existir."""
    if os.environ.get("RENDER") or os.environ.get("RENDER_EXTERNAL_URL"):
        return
    for base in [os.getcwd(), os.path.dirname(os.path.abspath(__file__))]:
        envpath = os.path.join(base, ".env")
        if not os.path.isfile(envpath):
            continue
        with open(envpath, encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("#") or "=" not in line:
                    continue
                if line.lower().startswith("export "):
                    line = line[7:].strip()
                key, val = line.split("=", 1)
                key = key.strip()
                val = val.strip()
                if " #" in val:
                    val = val.split(" #", 1)[0].rstrip()
                if " \t#" in val:
                    val = val.split("\t#", 1)[0].rstrip()
                val = val.strip().strip("'\"`")
                if key and val and key not in os.environ:
                    os.environ[key] = val
        print(f"  ğŸ“„ .env carregado: {envpath}")
        break


def _sanitize_env_value(value: Optional[str]) -> Optional[str]:
    if not value:
        return None
    return value.strip().strip("'\"`")


def _normalize_langfuse_env():
    public_key = _sanitize_env_value(os.environ.get("LANGFUSE_PUBLIC_KEY"))
    secret_key = _sanitize_env_value(os.environ.get("LANGFUSE_SECRET_KEY"))
    base_url = _sanitize_env_value(
        os.environ.get("LANGFUSE_BASE_URL") or os.environ.get("LANGFUSE_HOST")
    )
    if base_url:
        base_url = base_url.rstrip("/")
    if public_key:
        os.environ["LANGFUSE_PUBLIC_KEY"] = public_key
    if secret_key:
        os.environ["LANGFUSE_SECRET_KEY"] = secret_key
    if base_url:
        os.environ["LANGFUSE_BASE_URL"] = base_url
        os.environ.setdefault("LANGFUSE_HOST", base_url)


def main():
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘    ğŸ§  AGENTE ReAct MODERNO â€” OpenAI Function Calling (2025)      â•‘
    â•‘                                                                   â•‘
    â•‘  Arquitetura:                                                     â•‘
    â•‘                                                                   â•‘
    â•‘  OpenAI API â—„â”€â”€â”€â”€ function calling nativo (sem text parsing)     â•‘
    â•‘       â”‚            multi-turn natural (messages = scratchpad)     â•‘
    â•‘       â”‚            finish_reason guia o loop                     â•‘
    â•‘       â–¼                                                           â•‘
    â•‘  Agent â—„â”€â”€â”€â”€â–º MCP Client â—„â”€â”€â”€â”€â–º MCP Server (subprocess)         â•‘
    â•‘    â”‚                              â€¢ somar(a, b)                  â•‘
    â•‘    â”‚                              â€¢ subtrair(a, b)               â•‘
    â•‘    â–¼                                                              â•‘
    â•‘  LLM Compression (quando contexto > limite)                      â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    _load_dotenv()
    _normalize_langfuse_env()
    print("\n  ğŸ” ENV Langfuse:")
    print(f"     LANGFUSE_PUBLIC_KEY: {'ok' if os.environ.get('LANGFUSE_PUBLIC_KEY') else 'missing'}")
    print(f"     LANGFUSE_SECRET_KEY: {'ok' if os.environ.get('LANGFUSE_SECRET_KEY') else 'missing'}")
    print(f"     LANGFUSE_BASE_URL: {os.environ.get('LANGFUSE_BASE_URL', 'missing')}")

    api_key = os.environ.get("OPENAI_API_KEY", "")
    if not api_key:
        print("âŒ Configure OPENAI_API_KEY")
        print("   export OPENAI_API_KEY='sk-...'")
        print("\n   Ou cole aqui (Enter para cancelar):")
        api_key = input("   Key: ").strip()
        if not api_key:
            return

    agent = Agent(api_key)
    try:
        agent.setup()
        agent.run()
    finally:
        agent.cleanup()


if __name__ == "__main__":
    main()
